{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB4WAHoJsjBYrn2/d9qCL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hy30n80/DeepLearning/blob/main/nn_layers_pt.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g7Ap-osFJbzh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "nn_layers_pt.py\n",
        "\n",
        "PyTorch version of nn_layers\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numbers\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "function view_as_windows\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def view_as_windows(arr_in, window_shape, step=1):\n",
        "    \"\"\"Rolling window view of the input n-dimensional array.\n",
        "    Windows are overlapping views of the input array, with adjacent windows\n",
        "    shifted by a single row or column (or an index of a higher dimension).\n",
        "    Parameters\n",
        "    ----------\n",
        "    arr_in : Pytorch tensor\n",
        "        N-d Pytorch tensor.\n",
        "    window_shape : integer or tuple of length arr_in.ndim\n",
        "        Defines the shape of the elementary n-dimensional orthotope\n",
        "        (better know as hyperrectangle [1]_) of the rolling window view.\n",
        "        If an integer is given, the shape will be a hypercube of\n",
        "        sidelength given by its value.\n",
        "    step : integer or tuple of length arr_in.ndim\n",
        "        Indicates step size at which extraction shall be performed.\n",
        "        If integer is given, then the step is uniform in all dimensions.\n",
        "    Returns\n",
        "    -------\n",
        "    arr_out : ndarray\n",
        "        (rolling) window view of the input array.\n",
        "    Notes\n",
        "    -----\n",
        "    One should be very careful with rolling views when it comes to\n",
        "    memory usage.  Indeed, although a 'view' has the same memory\n",
        "    footprint as its base array, the actual array that emerges when this\n",
        "    'view' is used in a computation is generally a (much) larger array\n",
        "    than the original, especially for 2-dimensional arrays and above.\n",
        "    For example, let us consider a 3 dimensional array of size (100,\n",
        "    100, 100) of ``float64``. This array takes about 8*100**3 Bytes for\n",
        "    storage which is just 8 MB. If one decides to build a rolling view\n",
        "    on this array with a window of (3, 3, 3) the hypothetical size of\n",
        "    the rolling view (if one was to reshape the view for example) would\n",
        "    be 8*(100-3+1)**3*3**3 which is about 203 MB! The scaling becomes\n",
        "    even worse as the dimension of the input array becomes larger.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] https://en.wikipedia.org/wiki/Hyperrectangle\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import torch\n",
        "    >>> A = torch.arange(4*4).reshape(4,4)\n",
        "    >>> A\n",
        "    array([[ 0,  1,  2,  3],\n",
        "           [ 4,  5,  6,  7],\n",
        "           [ 8,  9, 10, 11],\n",
        "           [12, 13, 14, 15]])\n",
        "    >>> window_shape = (2, 2)\n",
        "    >>> B = view_as_windows(A, window_shape)\n",
        "    >>> B[0, 0]\n",
        "    array([[0, 1],\n",
        "           [4, 5]])\n",
        "    >>> B[0, 1]\n",
        "    array([[1, 2],\n",
        "           [5, 6]])\n",
        "    >>> A = torch.arange(10)\n",
        "    >>> A\n",
        "    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "    >>> window_shape = (3,)\n",
        "    >>> B = view_as_windows(A, window_shape)\n",
        "    >>> B.shape\n",
        "    (8, 3)\n",
        "    >>> B\n",
        "    array([[0, 1, 2],\n",
        "           [1, 2, 3],\n",
        "           [2, 3, 4],\n",
        "           [3, 4, 5],\n",
        "           [4, 5, 6],\n",
        "           [5, 6, 7],\n",
        "           [6, 7, 8],\n",
        "           [7, 8, 9]])\n",
        "    >>> A = torch.arange(5*4).reshape(5, 4)\n",
        "    >>> A\n",
        "    array([[ 0,  1,  2,  3],\n",
        "           [ 4,  5,  6,  7],\n",
        "           [ 8,  9, 10, 11],\n",
        "           [12, 13, 14, 15],\n",
        "           [16, 17, 18, 19]])\n",
        "    >>> window_shape = (4, 3)\n",
        "    >>> B = view_as_windows(A, window_shape)\n",
        "    >>> B.shape\n",
        "    (2, 2, 4, 3)\n",
        "    >>> B  # doctest: +NORMALIZE_WHITESPACE\n",
        "    array([[[[ 0,  1,  2],\n",
        "             [ 4,  5,  6],\n",
        "             [ 8,  9, 10],\n",
        "             [12, 13, 14]],\n",
        "            [[ 1,  2,  3],\n",
        "             [ 5,  6,  7],\n",
        "             [ 9, 10, 11],\n",
        "             [13, 14, 15]]],\n",
        "           [[[ 4,  5,  6],\n",
        "             [ 8,  9, 10],\n",
        "             [12, 13, 14],\n",
        "             [16, 17, 18]],\n",
        "            [[ 5,  6,  7],\n",
        "             [ 9, 10, 11],\n",
        "             [13, 14, 15],\n",
        "             [17, 18, 19]]]])\n",
        "    \"\"\"\n",
        "\n",
        "    # -- basic checks on arguments\n",
        "    if not torch.is_tensor(arr_in):\n",
        "        raise TypeError(\"`arr_in` must be a pytorch tensor\")\n",
        "\n",
        "    ndim = arr_in.ndim\n",
        "\n",
        "    if isinstance(window_shape, numbers.Number):\n",
        "        window_shape = (window_shape,) * ndim\n",
        "    if not (len(window_shape) == ndim):\n",
        "        raise ValueError(\"`window_shape` is incompatible with `arr_in.shape`\")\n",
        "\n",
        "    if isinstance(step, numbers.Number):\n",
        "        if step < 1:\n",
        "            raise ValueError(\"`step` must be >= 1\")\n",
        "        step = (step,) * ndim\n",
        "    if len(step) != ndim:\n",
        "        raise ValueError(\"`step` is incompatible with `arr_in.shape`\")\n",
        "\n",
        "    arr_shape = torch.tensor(arr_in.shape)\n",
        "    window_shape = torch.tensor(window_shape, dtype=arr_shape.dtype)\n",
        "\n",
        "    if ((arr_shape - window_shape) < 0).any():\n",
        "        raise ValueError(\"`window_shape` is too large\")\n",
        "\n",
        "    if ((window_shape - 1) < 0).any():\n",
        "        raise ValueError(\"`window_shape` is too small\")\n",
        "\n",
        "    # -- build rolling window view\n",
        "    slices = tuple(slice(None, None, st) for st in step)\n",
        "    # window_strides = torch.tensor(arr_in.stride())\n",
        "    window_strides = arr_in.stride()\n",
        "\n",
        "    indexing_strides = arr_in[slices].stride()\n",
        "\n",
        "    win_indices_shape = torch.div(arr_shape - window_shape\n",
        "                          , torch.tensor(step), rounding_mode = 'floor') + 1\n",
        "    \n",
        "    new_shape = tuple(list(win_indices_shape) + list(window_shape))\n",
        "    strides = tuple(list(indexing_strides) + list(window_strides))\n",
        "\n",
        "    arr_out = torch.as_strided(arr_in, size=new_shape, stride=strides)\n",
        "    return arr_out\n",
        "\n",
        "#######\n",
        "# if necessary, you can define additional functions which help your implementation,\n",
        "# and import proper libraries for those functions.\n",
        "#######\n",
        "\n",
        "class nn_convolutional_layer:\n",
        "\n",
        "    def __init__(self, f_height, f_width, input_size, in_ch_size, out_ch_size):\n",
        "        \n",
        "        # Xavier init\n",
        "        self.W = torch.normal(0, 1 / math.sqrt((in_ch_size * f_height * f_width / 2)),\n",
        "                                  size=(out_ch_size, in_ch_size, f_height, f_width))\n",
        "        self.b = 0.01 + torch.zeros(size=(1, out_ch_size, 1, 1))\n",
        "\n",
        "        self.W.requires_grad = True\n",
        "        self.b.requires_grad = True\n",
        "\n",
        "        self.v_W = torch.zeros_like(self.W)\n",
        "        self.v_b = torch.zeros_like(self.b)\n",
        "        \n",
        "        self.input_size = input_size\n",
        "\n",
        "    def update_weights(self, dW, db):\n",
        "        self.W += dW\n",
        "        self.b += db\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.W.clone().detach(), self.b.clone().detach()\n",
        "\n",
        "    def set_weights(self, W, b):\n",
        "        self.W = W.clone().detach()\n",
        "        self.b = b.clone().detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        ###################################\n",
        "        # Q4. Implement your layer here\n",
        "        \n",
        "\n",
        "        W = self.W\n",
        "        b = self.b\n",
        "\n",
        "        out_ch_size, in_ch_size, f_height, f_width = W.shape\n",
        "        batch_size, in_ch_size, input_height, input_width = x.shape\n",
        "\n",
        "        new_height =input_height - f_height+1\n",
        "        new_width = input_width - f_width+1\n",
        "\n",
        "\n",
        "        x_window = view_as_windows(x, window_shape = (1,in_ch_size, f_height, f_width), step=1)\n",
        "      \n",
        "        x_window = x_window.reshape(batch_size, new_height, new_width, -1)\n",
        "        W_s = W.reshape(out_ch_size,-1)\n",
        "\n",
        "        out = torch.matmul(x_window, W_s.T)\n",
        "        \n",
        "        output = out.permute(0, 3, 1, 2)+b\n",
        "\n",
        "        return output\n",
        "        \n",
        "        ###################################\n",
        "        \n",
        "    \n",
        "    def step(self, lr, friction):\n",
        "        with torch.no_grad():\n",
        "            self.v_W = friction*self.v_W + (1-friction)*self.W.grad\n",
        "            self.v_b = friction*self.v_b + (1-friction)*self.b.grad\n",
        "            self.W -= lr*self.v_W\n",
        "            self.b -= lr*self.v_b\n",
        "            \n",
        "            self.W.grad.zero_()\n",
        "            self.b.grad.zero_()\n",
        "\n",
        "# max pooling\n",
        "class nn_max_pooling_layer:\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.stride = stride\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size, in_ch_size, input_height, input_width = x.shape\n",
        "\n",
        "        new_height = (input_height-self.pool_size)//self.stride +1\n",
        "        new_width = (input_width-self.pool_size)//self.stride +1\n",
        "\n",
        "        x_window = view_as_windows(x, window_shape = (1,1,self.pool_size, self.pool_size), step = (1,1,self.stride,self.stride))\n",
        "        s_window = x_window.reshape(batch_size, in_ch_size, new_height, new_width, self.pool_size**2)\n",
        "        out,_ = torch.max(s_window, dim=-1)\n",
        "\n",
        "        return out\n",
        "        ###################################\n",
        "\n",
        "# relu activation\n",
        "class nn_activation_layer:\n",
        "\n",
        "    # linear layer. creates matrix W and bias b\n",
        "    # W is in by out, and b is out by 1\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.clamp(min=0)\n",
        "\n",
        "# fully connected (linear) layer\n",
        "class nn_fc_layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, std=1):\n",
        "        \n",
        "        # Xavier/He init\n",
        "        self.W = torch.normal(0, std/math.sqrt(input_size/2), (output_size, input_size))\n",
        "        self.b=0.01+torch.zeros((output_size))\n",
        "\n",
        "        self.W.requires_grad = True\n",
        "        self.b.requires_grad = True\n",
        "\n",
        "        self.v_W = torch.zeros_like(self.W)\n",
        "        self.v_b = torch.zeros_like(self.b)\n",
        "\n",
        "    ## Q1\n",
        "    def forward(self,x):\n",
        "        # compute forward pass of given parameter\n",
        "        # output size is batch x output_size x 1 x 1\n",
        "        # input size is batch x input_size x filt_size x filt_size\n",
        "        output_size = self.W.shape[0]\n",
        "        batch_size = x.shape[0]\n",
        "        #Wx=x.reshape((batch_size,-1))@(self.W.reshape(output_size,-1)).T\n",
        "        Wx = torch.mm(x.reshape((batch_size, -1)),(self.W.reshape(output_size, -1)).T)\n",
        "        out = Wx+self.b\n",
        "        return out\n",
        "\n",
        "    def update_weights(self,dLdW,dLdb):\n",
        "\n",
        "        # parameter update\n",
        "        self.W=self.W+dLdW\n",
        "        self.b=self.b+dLdb\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.W.clone().detach(), self.b.clone().detach()\n",
        "\n",
        "    def set_weights(self, W, b):\n",
        "        self.W = W.clone().detach()\n",
        "        self.b = b.clone().detach()\n",
        "    \n",
        "    def step(self, lr, friction):\n",
        "        with torch.no_grad():\n",
        "            self.v_W = friction*self.v_W + (1-friction)*self.W.grad\n",
        "            self.v_b = friction*self.v_b + (1-friction)*self.b.grad\n",
        "            self.W -= lr*self.v_W\n",
        "            self.b -= lr*self.v_b\n",
        "            self.W.grad.zero_()\n",
        "            self.b.grad.zero_()\n",
        "\n",
        "\n",
        "# softmax layer\n",
        "class nn_softmax_layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        s = x - torch.unsqueeze(torch.amax(x, axis=1), -1)\n",
        "        return (torch.exp(s) / torch.unsqueeze(torch.sum(torch.exp(s), axis=1), -1)).reshape((x.shape[0],x.shape[1]))\n",
        "\n",
        "\n",
        "# cross entropy layer\n",
        "class nn_cross_entropy_layer:\n",
        "    def __init__(self):\n",
        "        self.eps=1e-15\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # first get softmax\n",
        "        batch_size = x.shape[0]\n",
        "        num_class = x.shape[1]\n",
        "        # onehot = torch.zeros((batch_size, num_class))\n",
        "        # onehot[range(batch_size), list(y.reshape(-1))] = 1\n",
        "        onehot = np.zeros((batch_size, num_class))\n",
        "        onehot[range(batch_size), (np.array(y)).reshape(-1, )] = 1\n",
        "        onehot = torch.as_tensor(onehot)\n",
        "\n",
        "        # avoid numerial instability\n",
        "        x[x<self.eps]=self.eps\n",
        "        x=x/torch.unsqueeze(torch.sum(x,axis=1), -1)\n",
        "\n",
        "        return sum(-torch.sum(torch.log(x.reshape(batch_size, -1)) * onehot, axis=0)) / batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veOPh1mL8SYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}